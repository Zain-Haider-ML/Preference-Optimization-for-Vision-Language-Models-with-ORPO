{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NuSy1MgpiLS3"
      },
      "outputs": [],
      "source": [
        "# !pip install peft accelerate bitsandbytes\n",
        "!pip install --upgrade datasets fsspec trl wandb transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "from datasets import features\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "Z07El1deiogo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_name = \"openbmb/RLAIF-V-Dataset\"\n",
        "# model_name = \"liuhaotian/llava-v1.6-mistral-7b\"\n",
        "# model_name = 'llava-hf/llava-v1.6-mistral-7b-hf'\n",
        "# model_name = 'SurfaceData/llava-v1.6-mistral-7b-sglang'\n",
        "# model_name = 'google/paligemma-3b-pt-224'"
      ],
      "metadata": {
        "id": "EzcZAMUziwhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    ds = load_dataset(dataset_name, split=\"train[:10%]\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except ValueError as e:\n",
        "    print(f\"Failed to load dataset: {e}\")"
      ],
      "metadata": {
        "id": "ltkeMqokitt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8F5GgVFi77L",
        "outputId": "eecd8028-dd19-4b5d-d5da-ec8f98a45e45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['ds_name', 'image', 'question', 'chosen', 'rejected', 'origin_dataset', 'origin_split', 'idx', 'image_path'],\n",
              "    num_rows: 8313\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ds[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTA6FFFrkU6k",
        "outputId": "2a480fdb-9d35-494a-ad16-42daa001aaed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ds_name': 'RLAIF-V',\n",
              " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=L size=1024x848>,\n",
              " 'question': 'how many families?',\n",
              " 'chosen': 'The image shows a Union Organization table setup with 18,000 families.',\n",
              " 'rejected': 'The image does not provide any information about families.',\n",
              " 'origin_dataset': 'TextVQA',\n",
              " 'origin_split': '{\"model\": \"OmniLMM-12B\", \"feedback_model\": \"OmniLMM-12B\", \"type\": \"question_answering\"}',\n",
              " 'idx': 'OmniLMM-12B_OmniLMM-12B_1',\n",
              " 'image_path': 'TextVQA/train_images/8733d0a1351be922.jpg'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "from transformers import AutoModelForVision2Seq"
      ],
      "metadata": {
        "id": "1AwRe7jGO0HZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers BitsAndBytesConfig\n",
        "# from peft import get_peft_model, LoraConfig\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "# )"
      ],
      "metadata": {
        "id": "9zZCyM6cQnZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'HuggingFaceTB/SmolVLM-256M-Instruct'\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_name, do_image_splitting=False)\n",
        "model = AutoModelForVision2Seq.from_pretrained(model_name,\n",
        "                                              device_map=\"auto\", torch_dtype=torch.bfloat16)"
      ],
      "metadata": {
        "id": "zRtrL3EwMDvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lora_config = LoraConfig(\n",
        "#     r=8,\n",
        "#     lora_alpha=32,\n",
        "#     lora_dropout=0.1,\n",
        "#     bias=\"none\",\n",
        "#     target_modules=\"all-linear\" # Depends on model\n",
        "# )\n",
        "\n",
        "# model = get_peft_model(model, lora_config)\n",
        "# model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "uUCUOhRVL1wG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format(example):\n",
        "    # Prepare the input for the chat template\n",
        "    prompt = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": example[\"question\"]}],\n",
        "        },\n",
        "    ]\n",
        "    chosen = [\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": example[\"chosen\"]}],\n",
        "        },\n",
        "    ]\n",
        "    rejected = [\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": example[\"rejected\"]}],\n",
        "        },\n",
        "    ]\n",
        "    # Apply the chat template\n",
        "    prompt = processor.apply_chat_template(prompt, tokenize=False)\n",
        "    chosen = processor.apply_chat_template(chosen, tokenize=False)\n",
        "    rejected = processor.apply_chat_template(rejected, tokenize=False)\n",
        "    # Resize the image to ensure it fits within the maximum allowable\n",
        "    # size of the processor to prevent OOM errors.\n",
        "    max_size = processor.image_processor.size[\"longest_edge\"]\n",
        "    example[\"image\"].thumbnail((max_size, max_size))\n",
        "    return {\"images\": [example[\"image\"]], \"prompt\": prompt, \"chosen\": chosen, \"rejected\": rejected}\n",
        "\n",
        "dataset = ds.map(format, remove_columns=ds.column_names)\n"
      ],
      "metadata": {
        "id": "wUWwI-ObxhDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTc0NYC9Lepr",
        "outputId": "f2184ba3-a523-4431-aaa6-50c3b8ee5dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'chosen': '<|im_start|>Assistant: The image shows a Union Organization table setup with 18,000 families.<end_of_utterance>\\n', 'rejected': '<|im_start|>Assistant: The image does not provide any information about families.<end_of_utterance>\\n', 'images': [<PIL.JpegImagePlugin.JpegImageFile image mode=L size=1024x848 at 0x7D7F7604A890>], 'prompt': '<|im_start|>User:<image>how many families?<end_of_utterance>\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure that the images are decoded, it prevents from storing bytes.\n",
        "f = dataset.features\n",
        "f[\"images\"] = features.Sequence(features.Image(decode=True))  # to avoid bytes\n",
        "dataset = dataset.cast(f)\n",
        "\n",
        "# Split the dataset\n",
        "splits = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Access the splits\n",
        "train_dataset = splits[\"train\"]\n",
        "test_dataset = splits[\"test\"]\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "VSfgRhjQEZvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import ORPOConfig, ORPOTrainer"
      ],
      "metadata": {
        "id": "xUs7xYYTA3dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "a6EppOFhYZAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = ORPOConfig(\n",
        "    output_dir=\"smolvlm_finetuned\",\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=1e-5,  # ↑ changed\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_ratio=0.03,  # ↑ added\n",
        "    num_train_epochs=5,  # ↓ reduced\n",
        "    logging_steps=50,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": True},\n",
        "    remove_unused_columns=False,\n",
        "    max_prompt_length=512,\n",
        "    max_length=1024,\n",
        "    report_to=\"wandb\",\n",
        "    do_eval=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\"\n",
        ")"
      ],
      "metadata": {
        "id": "oxEOcJ4bBBPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOaTZjbOypBM",
        "outputId": "65abb748-1bda-49b1-f46e-4c6ab3e214ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['chosen', 'rejected', 'images', 'prompt'],\n",
              "     num_rows: 7481\n",
              " }),\n",
              " Dataset({\n",
              "     features: ['chosen', 'rejected', 'images', 'prompt'],\n",
              "     num_rows: 832\n",
              " }))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = ORPOTrainer(model=model, args=config, train_dataset=train_dataset,\n",
        "                      processing_class = processor.tokenizer, eval_dataset = test_dataset)"
      ],
      "metadata": {
        "id": "eph6zgqUCH9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "fZ_yyVpoC-8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = '/content/ft_smolvlm'\n",
        "\n",
        "model.save_pretrained(save_dir)\n",
        "processor.save_pretrained(save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJ6e6JJdp_YS",
        "outputId": "69a01f37-676a-4621-bb74-637d4f8e2e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/ft_smolvlm/processor_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization"
      ],
      "metadata": {
        "id": "zCZXwzEbgc3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install quanto\n",
        "!pip install git+https://github.com/huggingface/accelerate.git\n",
        "!pip install git+https://github.com/huggingface/transformers.git\n",
        "!pip install optimum-quanto"
      ],
      "metadata": {
        "id": "SzMDCJop1q9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor\n",
        "from transformers import AutoModelForVision2Seq, QuantoConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "IGICYk-b8HgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_id = \"/content/ft_smolvlm\"\n",
        "\n",
        "ft_processor = AutoProcessor.from_pretrained(model_id)\n",
        "\n",
        "quantization_config = QuantoConfig(weights=\"int8\")\n",
        "\n",
        "quantized_model = AutoModelForVision2Seq.from_pretrained(model_id, device_map=\"auto\", quantization_config=quantization_config)"
      ],
      "metadata": {
        "id": "CIDu87Ev4iK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(quantized_model)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4Pjb0t598r38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mmbA0a-h9Yc8",
        "outputId": "f819d08a-53ff-4761-bb12-ecf510d2e581"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['ds_name', 'image', 'question', 'chosen', 'rejected', 'origin_dataset', 'origin_split', 'idx', 'image_path'],\n",
              "    num_rows: 8313\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = np.random.randint(0, len(ds))\n",
        "\n",
        "user_message = ds[s]['question']\n",
        "image = ds[s]['image']\n",
        "\n",
        "print(\"Sample number: \",s, \"\\nQuestion: \" + user_message,)"
      ],
      "metadata": {
        "id": "jR_wx9Ii2wiu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cfd6920-0cf0-4954-edca-816116951812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample number:  7971 \n",
            "Question: How many signs are there?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image"
      ],
      "metadata": {
        "id": "u5Exhlk391xO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": user_message}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Prepare inputs\n",
        "prompt = ft_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "print(\"Prompt : \", prompt, \"\\n\")\n",
        "inputs = ft_processor(text=prompt, images=[image], return_tensors=\"pt\")\n",
        "\n",
        "inputs = inputs.to(DEVICE)\n",
        "\n",
        "\n",
        "# Generate outputs\n",
        "generated_ids = quantized_model.generate(**inputs, max_new_tokens=250)\n",
        "generated_texts = ft_processor.batch_decode(\n",
        "    generated_ids,\n",
        "    skip_special_tokens=True,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgnwUpr2nc_K",
        "outputId": "610272bd-6ff4-415c-9ac0-338de237440e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt :  <|im_start|>User:<image>How many signs are there?<end_of_utterance>\n",
            "Assistant: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_texts[0])"
      ],
      "metadata": {
        "id": "Vjc07C1kn7Nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87983e77-994e-41e0-b87f-e9838fb42153"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User:How many signs are there?\n",
            "Assistant: There are four signs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hosting"
      ],
      "metadata": {
        "id": "gWUChrVk_uCu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "87x7ex_J_tu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "TTKxH7WD_9wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def inference(image, user_message):\n",
        "    # Compose the multimodal prompt as required by your processor\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},\n",
        "                {\"type\": \"text\", \"text\": user_message}\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # Apply chat template to format the input prompt\n",
        "    prompt = ft_processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "\n",
        "    # Process the prompt and image to model inputs\n",
        "    inputs = ft_processor(text=prompt, images=[image], return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    # Generate outputs from the quantized model\n",
        "    generated_ids = quantized_model.generate(**inputs, max_new_tokens=250)\n",
        "\n",
        "    # Decode output tokens to text\n",
        "    generated_texts = ft_processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return generated_texts[0]  # Return the first generated text\n"
      ],
      "metadata": {
        "id": "xLZzuWTf_p7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface = gr.Interface(\n",
        "    fn=inference,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"Input Image\"),\n",
        "        gr.Textbox(lines=2, placeholder=\"Ask something about the image...\", label=\"User Message\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Model Response\"),\n",
        "    title=\"(Quantized) Multimodal Chat\",\n",
        "    description=\"Upload an image and ask a question to get a response from the model.\",\n",
        "    allow_flagging=\"never\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "E_3lFHnM-fZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iface.launch(debug=True)"
      ],
      "metadata": {
        "id": "LMEwmevkA2nv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}